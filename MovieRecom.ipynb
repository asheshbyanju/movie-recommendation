{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is adopted from [this databricks lab](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2799933550853697/2823893187441173/2202577924924539/latest.html) and is customized for AWS Educate.\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-nd/4.0/\"> <img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png\"/> </a> <br/> This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-nd/4.0/\"> Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License. </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png) + ![Python Logo](http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png)\n",
    "\n",
    "<img src=\"http://spark-mooc.github.io/web-assets/images/cs110x/movie-camera.png\" style=\"float:right; height: 200px; margin: 10px; border: 1px solid #ddd; border-radius: 15px 15px 15px 15px; padding: 10px\"/>\n",
    "\n",
    "# Predicting Movie Ratings\n",
    "\n",
    "One of the most common uses of big data is to predict what users want.  This allows Google to show you relevant ads, Amazon to recommend relevant products, and Netflix to recommend movies that you might like.  This lab will demonstrate how we can use Apache Spark to recommend movies to a user.  We will start with some basic techniques, and then use the [Spark ML][sparkml] library's Alternating Least Squares method to make more sophisticated predictions.\n",
    "\n",
    "In this project, we will use MLlib to make personalized movie recommendations tailored _for you_. We will work with 10 million ratings from 72,000 users on 10,000 movies, collected by [MovieLens](https://grouplens.org/datasets/movielens/). This dataset is can be found at https://grouplens.org/datasets/movielens/latest/. You may want to start with a smaller version of the dataset. Both datasets are also available on Blackboard. \n",
    "\n",
    "Note that the current Notebook contains various sections marked with **ToDO** and **FILL_IN**. These are the graded components of this project and need to be completed by you.\n",
    "\n",
    "In this lab:\n",
    "* *Part 0*: Preliminaries\n",
    "* *Part 1*: Basic Recommendations\n",
    "* *Part 2*: Collaborative Filtering\n",
    "* *Part 3*: Predictions for Yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n",
    "\n",
    "This assignment can be completed using basic Python and pySpark DataFrame Transformations and Actions. With the exception of the ML functions that we introduce in this assignment, you should be able to complete all parts of this homework using only the Spark functions you have used in prior projects (although you are welcome to use more features of Spark if you like!).\n",
    "\n",
    " We will work with 10 million ratings from 72,000 users on 10,000 movies, collected by [MovieLens](https://grouplens.org/datasets/movielens/). This dataset is can be found at https://grouplens.org/datasets/movielens/latest/. You may want to start with a smaller version of the dataset. Both datasets are also available on Blackboard. \n",
    " \n",
    "You will need to Spawn a Spark cluster and Jupyter Notebook server using the instructions provided in Sparkify 8, and upload a copy of this notebook to this Spark Notebook server. Also, you will have to upload the dataset (i.e., CSV files) on S3.  The following cell defines the locations of the data files. You'll need to adjust the paths, below.\n",
    "\n",
    "**To Do**: Run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using small data\n",
    "\n",
    "import os\n",
    "# from test_helpers import Test\n",
    "import test_helpers \n",
    "# from test_helpers import test\n",
    "\n",
    "# Change to the location of data files\n",
    "dbfs_dir = 's3://dsci6007f20movierecommendation/ml-latest-small'\n",
    "ratings_filename = dbfs_dir + '/ratings.csv'\n",
    "movies_filename = dbfs_dir + '/movies.csv'\n",
    "\n",
    "movies_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://dsci6007f20movierecommendation/ml-latest/movies.csv'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # using all data\n",
    "\n",
    "# import os\n",
    "# # from test_helpers import Test\n",
    "# import test_helpers \n",
    "# # from test_helpers import test\n",
    "\n",
    "# # Change to the location of data files\n",
    "# dbfs_dir = 's3://dsci6007f20movierecommendation/ml-latest'\n",
    "# ratings_filename = dbfs_dir + '/ratings.csv'\n",
    "# movies_filename = dbfs_dir + '/movies.csv'\n",
    "\n",
    "# movies_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Preliminaries\n",
    "\n",
    "We read in each of the files and create a DataFrame consisting of parsed lines.\n",
    "\n",
    "### The 20-million movie sample\n",
    "\n",
    "The 20-million movie sample consists of CSV files (with headers), so there's no need to parse the files manually, as Spark CSV can do the job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU vs I/O tradeoff\n",
    "\n",
    "Note that we have both compressed files (ending in `.gz`) and uncompressed files. We have a CPU vs. I/O tradeoff here. If I/O is the bottleneck, then we want to process the compressed files and pay the extra CPU overhead. If CPU is the bottleneck, then it makes more sense to process the uncompressed files.\n",
    "\n",
    "We've done some experiments, and we've determined that CPU is more of a bottleneck than I/O. So, we're going to process the uncompressed data. In addition, we're going to speed things up further by specifying the DataFrame schema explicitly. (When the Spark CSV adapter infers the schema from a CSV file, it has to make an extra pass over the file. That'll slow things down here, and it isn't really necessary.)\n",
    "\n",
    "**To Do**: Run the following cell, which will define the schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "ratings_df_schema = StructType(\n",
    "  [StructField('userId', IntegerType()),\n",
    "   StructField('movieId', IntegerType()),\n",
    "   StructField('rating', DoubleType())]\n",
    ")\n",
    "movies_df_schema = StructType(\n",
    "  [StructField('ID', IntegerType()),\n",
    "   StructField('title', StringType())]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Cache\n",
    "\n",
    "By now, your datasets should be hosted on S3. We're going to be accessing this data a lot. Rather than read it over and over again from S3, we'll cache both the movies DataFrame and the ratings DataFrame in memory.\n",
    "\n",
    "**To Do**: Run the following cell to load and cache the data. Please be patient: The code takes about 30 seconds to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 27753444 ratings and 58098 movies in the datasets\n",
      "Ratings:\n",
      "+------+-------+------+\n",
      "|userId|movieId|rating|\n",
      "+------+-------+------+\n",
      "|     1|    307|   3.5|\n",
      "|     1|    481|   3.5|\n",
      "|     1|   1091|   1.5|\n",
      "+------+-------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Movies:\n",
      "+---+-----------------------+\n",
      "|ID |title                  |\n",
      "+---+-----------------------+\n",
      "|1  |Toy Story (1995)       |\n",
      "|2  |Jumanji (1995)         |\n",
      "|3  |Grumpier Old Men (1995)|\n",
      "+---+-----------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "raw_ratings_df = sqlContext.read.format('csv').options(header=True, inferSchema=False).schema(ratings_df_schema).load(ratings_filename)\n",
    "ratings_df = raw_ratings_df.drop('Timestamp')\n",
    "\n",
    "raw_movies_df = sqlContext.read.format('csv').options(header=True, inferSchema=False).schema(movies_df_schema).load(movies_filename)\n",
    "movies_df = raw_movies_df.drop('Genres').withColumnRenamed('movieId', 'ID')\n",
    "\n",
    "ratings_df.cache()\n",
    "movies_df.cache()\n",
    "\n",
    "assert ratings_df.is_cached\n",
    "assert movies_df.is_cached\n",
    "\n",
    "raw_ratings_count = raw_ratings_df.count()\n",
    "ratings_count = ratings_df.count()\n",
    "raw_movies_count = raw_movies_df.count()\n",
    "movies_count = movies_df.count()\n",
    "\n",
    "print('There are %s ratings and %s movies in the datasets' % (ratings_count, movies_count))\n",
    "print('Ratings:')\n",
    "ratings_df.show(3)\n",
    "print('Movies:')\n",
    "movies_df.show(3, truncate=False)\n",
    "\n",
    "assert raw_ratings_count == ratings_count\n",
    "assert raw_movies_count == movies_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's do a quick verification of the data.\n",
    "\n",
    "**To do**: Run the following cell. It should run without errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at some of the data in the two DataFrames.\n",
    "\n",
    "**To Do**: Run the following two cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| ID|               title|\n",
      "+---+--------------------+\n",
      "|  1|    Toy Story (1995)|\n",
      "|  2|      Jumanji (1995)|\n",
      "|  3|Grumpier Old Men ...|\n",
      "|  4|Waiting to Exhale...|\n",
      "|  5|Father of the Bri...|\n",
      "|  6|         Heat (1995)|\n",
      "|  7|      Sabrina (1995)|\n",
      "|  8| Tom and Huck (1995)|\n",
      "|  9| Sudden Death (1995)|\n",
      "| 10|    GoldenEye (1995)|\n",
      "+---+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+\n",
      "|userId|movieId|rating|\n",
      "+------+-------+------+\n",
      "|     1|    307|   3.5|\n",
      "|     1|    481|   3.5|\n",
      "|     1|   1091|   1.5|\n",
      "|     1|   1257|   4.5|\n",
      "|     1|   1449|   4.5|\n",
      "|     1|   1590|   2.5|\n",
      "|     1|   1591|   1.5|\n",
      "|     1|   2134|   4.5|\n",
      "|     1|   2478|   4.0|\n",
      "|     1|   2840|   3.0|\n",
      "+------+-------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Basic Recommendations\n",
    "\n",
    "One way to recommend movies is to always recommend the movies with the highest average rating. In this part, we will use Spark to find the name, number of ratings, and the average rating of the 20 movies with the highest average rating and at least 500 reviews. We want to filter our movies with high ratings but greater than or equal to 500 reviews because movies with few reviews may not have broad appeal to everyone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1a) Movies with Highest Average Ratings\n",
    "\n",
    "Let's determine the movies with the highest average ratings.\n",
    "\n",
    "The steps you should perform are:\n",
    "\n",
    "1. Recall that the `ratings_df` contains three columns:\n",
    "    - The ID of the user who rated the film\n",
    "    - the ID of the movie being rated\n",
    "    - and the rating.\n",
    "\n",
    "   First, transform `ratings_df` into a second DataFrame, `movie_ids_with_avg_ratings`, with the following columns:\n",
    "    - The movie ID\n",
    "    - The number of ratings for the movie\n",
    "    - The average of all the movie's ratings\n",
    "\n",
    "2. Transform `movie_ids_with_avg_ratings` to another DataFrame, `movie_names_with_avg_ratings_df` that adds the movie name to each row. `movie_names_with_avg_ratings_df`\n",
    "   will contain these columns:\n",
    "    - The movie ID\n",
    "    - The movie name\n",
    "    - The number of ratings for the movie\n",
    "    - The average of all the movie's ratings\n",
    "\n",
    "   **Hint**: You'll need to do a join.\n",
    "\n",
    "You should end up with something like the following:\n",
    "```\n",
    "movie_ids_with_avg_ratings_df:\n",
    "+-------+-----+------------------+\n",
    "|movieId|count|average           |\n",
    "+-------+-----+------------------+\n",
    "|1831   |7463 |2.5785207021305103|\n",
    "|431    |8946 |3.695059244355019 |\n",
    "|631    |2193 |2.7273141814865483|\n",
    "+-------+-----+------------------+\n",
    "only showing top 3 rows\n",
    "\n",
    "movie_names_with_avg_ratings_df:\n",
    "+-------+-----------------------------+-----+-------+\n",
    "|average|title                        |count|movieId|\n",
    "+-------+-----------------------------+-----+-------+\n",
    "|5.0    |Ella Lola, a la Trilby (1898)|1    |94431  |\n",
    "|5.0    |Serving Life (2011)          |1    |129034 |\n",
    "|5.0    |Diplomatic Immunity (2009? ) |1    |107434 |\n",
    "+-------+-----------------------------+-----+-------+\n",
    "only showing top 3 rows\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie_ids_with_avg_ratings_df:\n",
      "+-------+-----+------------------+\n",
      "|movieId|count|average           |\n",
      "+-------+-----+------------------+\n",
      "|1591   |6508 |2.6466656422864165|\n",
      "|1088   |14100|3.2480141843971633|\n",
      "|2122   |2908 |2.6475240715268225|\n",
      "+-------+-----+------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "movie_names_with_avg_ratings_df:\n",
      "+------------------+---------------------------+-----+-------+\n",
      "|average           |title                      |count|movieId|\n",
      "+------------------+---------------------------+-----+-------+\n",
      "|2.6466656422864165|Spawn (1997)               |6508 |1591   |\n",
      "|3.2480141843971633|Dirty Dancing (1987)       |14100|1088   |\n",
      "|2.6475240715268225|Children of the Corn (1984)|2908 |2122   |\n",
      "+------------------+---------------------------+-----+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL_IN> with appropriate code\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# From ratingsDF, create a movie_ids_with_avg_ratings_df that combines the two DataFrames\n",
    "movie_ids_with_avg_ratings_df = ratings_df.groupBy('movieId').agg(F.count(ratings_df.rating).alias(\"count\"), F.avg(ratings_df.rating).alias(\"average\"))\n",
    "print('movie_ids_with_avg_ratings_df:')\n",
    "movie_ids_with_avg_ratings_df.show(3, truncate=False)\n",
    "\n",
    "# Note: movie_names_df is a temporary variable, used only to separate the steps necessary\n",
    "# to create the movie_names_with_avg_ratings_df DataFrame.\n",
    "movie_names_df = movie_ids_with_avg_ratings_df.join(movies_df, movie_ids_with_avg_ratings_df.movieId==movies_df.ID)\n",
    "movie_names_with_avg_ratings_df = movie_names_df.select('average', 'title', 'count', 'movieId')\n",
    "\n",
    "print('movie_names_with_avg_ratings_df:')\n",
    "movie_names_with_avg_ratings_df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+------------------+------------------+\n",
      "|summary|           average|               title|             count|           movieId|\n",
      "+-------+------------------+--------------------+------------------+------------------+\n",
      "|  count|             53889|               53889|             53889|             53889|\n",
      "|   mean|3.0685927253973184|                null| 515.0113010076268|109110.13795023103|\n",
      "| stddev|0.7362424202405309|                null|2934.7589389155864| 60910.55168607507|\n",
      "|    min|               0.5|\"\"\"Great Performa...|                 1|                 1|\n",
      "|    max|               5.0|     줄탁동시 (2012)|             97999|            193886|\n",
      "+-------+------------------+--------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movie_names_with_avg_ratings_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1b) Movies with Highest Average Ratings and at least 500 reviews\n",
    "\n",
    "Now that we have a DataFrame of the movies with highest average ratings, we can use Spark to determine the 20 movies with highest average ratings and at least 500 reviews.\n",
    "\n",
    "Add a single DataFrame transformation (in place of `<FILL_IN>`, below) to limit the results to movies with ratings from at least 500 people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies with highest ratings:\n",
      "+------------------+---------------------------------------------------------------------------+-----+-------+\n",
      "|average           |title                                                                      |count|movieId|\n",
      "+------------------+---------------------------------------------------------------------------+-----+-------+\n",
      "|4.4865181711606095|Planet Earth II (2016)                                                     |853  |171011 |\n",
      "|4.458092485549133 |Planet Earth (2006)                                                        |1384 |159817 |\n",
      "|4.424188001918387 |Shawshank Redemption, The (1994)                                           |97999|318    |\n",
      "|4.399898373983739 |Band of Brothers (2001)                                                    |984  |170705 |\n",
      "|4.350558659217877 |Black Mirror: White Christmas (2014)                                       |1074 |174053 |\n",
      "|4.332892749244713 |Godfather, The (1972)                                                      |60904|858    |\n",
      "|4.291958829205532 |Usual Suspects, The (1995)                                                 |62180|50     |\n",
      "|4.2630353697749195|Godfather: Part II, The (1974)                                             |38875|1221   |\n",
      "|4.257501817775044 |Schindler's List (1993)                                                    |71516|527    |\n",
      "|4.2541157909178215|Seven Samurai (Shichinin no samurai) (1954)                                |14578|2019   |\n",
      "|4.237075455914338 |12 Angry Men (1957)                                                        |17931|1203   |\n",
      "|4.230798598634567 |Rear Window (1954)                                                         |22264|904    |\n",
      "|4.230663235786717 |Fight Club (1999)                                                          |65678|2959   |\n",
      "|4.222920272160452 |One Flew Over the Cuckoo's Nest (1975)                                     |42181|1193   |\n",
      "|4.210098086509085 |Casablanca (1942)                                                          |31095|912    |\n",
      "|4.208876000542667 |Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1964)|29484|750    |\n",
      "|4.2076678004047015|Spirited Away (Sen to Chihiro no kamikakushi) (2001)                       |23227|5618   |\n",
      "|4.20375939849624  |Third Man, The (1949)                                                      |7980 |1212   |\n",
      "|4.202861952861953 |Whiplash (2013)                                                            |594  |166024 |\n",
      "|4.201752440106477 |Paths of Glory (1957)                                                      |4508 |1178   |\n",
      "+------------------+---------------------------------------------------------------------------+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# For test data \n",
    "# movies_with_500_ratings_or_more = movie_names_with_avg_ratings_df.filter('count >=50').sort('average', ascending=False)\n",
    "\n",
    "movies_with_500_ratings_or_more = movie_names_with_avg_ratings_df.filter('count >=500').sort('average', ascending=False)\n",
    "\n",
    "print('Movies with highest ratings:')\n",
    "movies_with_500_ratings_or_more.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a threshold on the number of reviews is one way to improve the recommendations, but there are many other good ways to improve quality. For example, you could weight ratings by the number of ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Collaborative Filtering\n",
    "In this course, you have learned about many of the basic transformations and actions that Spark allows us to apply to distributed datasets.  Spark also exposes some higher level functionality; in particular, Machine Learning using a component of Spark called [MLlib][mllib].  In this part, you will learn how to use MLlib to make personalized movie recommendations using the movie data we have been analyzing.\n",
    "\n",
    "<img src=\"https://courses.edx.org/c4x/BerkeleyX/CS100.1x/asset/Collaborative_filtering.gif\" alt=\"collaborative filtering\" style=\"float: right\"/>\n",
    "\n",
    "We are going to use a technique called [collaborative filtering][collab]. Collaborative filtering is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating). The underlying assumption of the collaborative filtering approach is that if a person A has the same opinion as a person B on an issue, A is more likely to have B's opinion on a different issue x than to have the opinion on x of a person chosen randomly. You can read more about collaborative filtering [here][collab2].\n",
    "\n",
    "The image at the right (from [Wikipedia][collab]) shows an example of predicting of the user's rating using collaborative filtering. At first, people rate different items (like videos, images, games). After that, the system is making predictions about a user's rating for an item, which the user has not rated yet. These predictions are built upon the existing ratings of other users, who have similar ratings with the active user. For instance, in the image below the system has made a prediction, that the active user will not like the video.\n",
    "\n",
    "<br clear=\"all\"/>\n",
    "\n",
    "----\n",
    "\n",
    "For movie recommendations, we start with a matrix whose entries are movie ratings by users (shown in red in the diagram below).  Each column represents a user (shown in green) and each row represents a particular movie (shown in blue).\n",
    "\n",
    "Since not all users have rated all movies, we do not know all of the entries in this matrix, which is precisely why we need collaborative filtering.  For each user, we have ratings for only a subset of the movies.  With collaborative filtering, the idea is to approximate the ratings matrix by factorizing it as the product of two matrices: one that describes properties of each user (shown in green), and one that describes properties of each movie (shown in blue).\n",
    "\n",
    "<img alt=\"factorization\" src=\"http://spark-mooc.github.io/web-assets/images/matrix_factorization.png\" style=\"width: 885px\"/>\n",
    "<br clear=\"all\"/>\n",
    "\n",
    "We want to select these two matrices such that the error for the users/movie pairs where we know the correct ratings is minimized.  The [Alternating Least Squares][als] algorithm does this by first randomly filling the users matrix with values and then optimizing the value of the movies such that the error is minimized.  Then, it holds the movies matrix constant and optimizes the value of the user's matrix.  This alternation between which matrix to optimize is the reason for the \"alternating\" in the name.\n",
    "\n",
    "This optimization is what's being shown on the right in the image above.  Given a fixed set of user factors (i.e., values in the users matrix), we use the known ratings to find the best values for the movie factors using the optimization written at the bottom of the figure.  Then we \"alternate\" and pick the best user factors given fixed movie factors.\n",
    "\n",
    "[als]: https://en.wikiversity.org/wiki/Least-Squares_Method\n",
    "[mllib]: http://spark.apache.org/docs/latest/mllib-guide.html\n",
    "[collab]: https://en.wikipedia.org/?title=Collaborative_filtering\n",
    "[collab2]: http://recommender-systems.org/collaborative-filtering/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2a) Creating a Training Set\n",
    "\n",
    "Before we jump into using machine learning, we need to break up the `ratings_df` dataset into three pieces:\n",
    "* A training set (DataFrame), which we will use to train models\n",
    "* A validation set (DataFrame), which we will use to choose the best model\n",
    "* A test set (DataFrame), which we will use for our experiments\n",
    "\n",
    "To randomly split the dataset into the multiple groups, we can use the pySpark [randomSplit()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.randomSplit) transformation. `randomSplit()` takes a set of splits and a seed and returns multiple DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 16649719, validation: 5554029, test: 5549696\n",
      "\n",
      "+------+-------+------+\n",
      "|userId|movieId|rating|\n",
      "+------+-------+------+\n",
      "|     1|    307|   3.5|\n",
      "|     1|    481|   3.5|\n",
      "|     1|   1257|   4.5|\n",
      "+------+-------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+------+-------+------+\n",
      "|userId|movieId|rating|\n",
      "+------+-------+------+\n",
      "|     1|   1091|   1.5|\n",
      "|     1|   2478|   4.0|\n",
      "|     1|   2986|   2.5|\n",
      "+------+-------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+------+-------+------+\n",
      "|userId|movieId|rating|\n",
      "+------+-------+------+\n",
      "|     1|   1590|   2.5|\n",
      "|     1|   1591|   1.5|\n",
      "|     1|   3424|   4.5|\n",
      "+------+-------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL_IN> with the appropriate code.\n",
    "\n",
    "# We'll hold out 60% for training, 20% of our data for validation, and leave 20% for testing\n",
    "seed = 1800009193\n",
    "(split_60_df, split_a_20_df, split_b_20_df) = ratings_df.randomSplit([0.6,0.2,0.2], seed=seed)\n",
    "\n",
    "# Let's cache these datasets for performance\n",
    "training_df = split_60_df.cache()\n",
    "validation_df = split_a_20_df.cache()\n",
    "test_df = split_b_20_df.cache()\n",
    "\n",
    "print('Training: {0}, validation: {1}, test: {2}\\n'.format(\n",
    "  training_df.count(), validation_df.count(), test_df.count())\n",
    ")\n",
    "training_df.show(3)\n",
    "validation_df.show(3)\n",
    "test_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27753444"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After splitting the dataset, your training set has about 12 million entries and the validation and test sets each have about 4 million entries. (The exact number of entries in each dataset varies slightly due to the random nature of the `randomSplit()` transformation.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2b) Alternating Least Squares\n",
    "\n",
    "In this part, we will use the Apache Spark ML Pipeline implementation of Alternating Least Squares, [ALS](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.recommendation.ALS). ALS takes a training dataset (DataFrame) and several parameters that control the model creation process. To determine the best values for the parameters, we will use ALS to train several models, and then we will select the best model and use the parameters from that model in the rest of this lab exercise.\n",
    "\n",
    "The process we will use for determining the best model is as follows:\n",
    "1. Pick a set of model parameters. The most important parameter to model is the *rank*, which is the number of columns in the Users matrix (green in the diagram above) or the number of rows in the Movies matrix (blue in the diagram above). In general, a lower rank will mean higher error on the training dataset, but a high rank may lead to [overfitting](https://en.wikipedia.org/wiki/Overfitting).  We will train models with ranks of 4, 8, and 12 using the `training_df` dataset.\n",
    "\n",
    "2. Set the appropriate parameters on the `ALS` object:\n",
    "    * The \"User\" column will be set to the values in our `userId` DataFrame column.\n",
    "    * The \"Item\" column will be set to the values in our `movieId` DataFrame column.\n",
    "    * The \"Rating\" column will be set to the values in our `rating` DataFrame column.\n",
    "    * We'll using a regularization parameter of 0.1.\n",
    "\n",
    "   **Note**: Read the documentation for the [ALS](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.recommendation.ALS) class **carefully**. It will help you accomplish this step.\n",
    "3. Have the ALS output transformation (i.e., the result of [ALS.fit()](http://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.recommendation.ALS.fit)) produce a _new_ column\n",
    "   called \"prediction\" that contains the predicted value.\n",
    "\n",
    "4. Create multiple models using [ALS.fit()](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.recommendation.ALS.fit), one for each of our rank values. We'll fit\n",
    "   against the training data set (`training_df`).\n",
    "\n",
    "5. For each model, we'll run a prediction against our validation data set (`validation_df`) and check the error.\n",
    "\n",
    "6. We'll keep the model with the best error rate.\n",
    "\n",
    "#### Why are we doing our own cross-validation?\n",
    "\n",
    "A challenge for collaborative filtering is how to provide ratings to a new user (a user who has not provided *any* ratings at all). Some recommendation systems choose to provide new users with a set of default ratings (e.g., an average value across all ratings), while others choose to provide no ratings for new users. Spark's ALS algorithm yields a NaN (`Not a Number`) value when asked to provide a rating for a new user.\n",
    "\n",
    "Using the ML Pipeline's [CrossValidator](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator) with ALS is thus problematic, because cross validation involves dividing the training data into a set of folds (e.g., three sets) and then using those folds for testing and evaluating the parameters during the parameter grid search process. It is likely that some of the folds will contain users that are not in the other folds, and, as a result, ALS produces NaN values for those new users. When the CrossValidator uses the Evaluator (RMSE) to compute an error metric, the RMSE algorithm will return NaN. This will make *all* of the parameters in the parameter grid appear to be equally good (or bad).\n",
    "\n",
    "You can read the discussion on [Spark JIRA 14489](https://issues.apache.org/jira/browse/SPARK-14489) about this issue. There are proposed workarounds of having ALS provide default values or having RMSE drop NaN values. Both introduce potential issues. We have chosen to have RMSE drop NaN values. While this does not solve the underlying issue of ALS not predicting a value for a new user, it does provide some evaluation value. We manually implement the parameter grid search process using a for loop (below) and remove the NaN values before using RMSE.\n",
    "\n",
    "For a production application, you would want to consider the tradeoffs in how to handle new users.\n",
    "\n",
    "**Note**: This cell will likely take a couple of minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rank 4 the RMSE is 0.8397051984739614\n",
      "For rank 8 the RMSE is 0.8228419462536534\n",
      "For rank 12 the RMSE is 0.8253626240871798\n",
      "The best model was trained with rank 8\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# This step is broken in ML Pipelines: https://issues.apache.org/jira/browse/SPARK-14489\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "# Let's initialize our ALS learner\n",
    "als = ALS()\n",
    "\n",
    "# Now we set the parameters for the method\n",
    "als.setMaxIter(5)\\\n",
    "   .setSeed(seed)\\\n",
    "   .setRegParam(0.1)\\\n",
    "   .setUserCol('userId')\\\n",
    "   .setItemCol('movieId')\\\n",
    "   .setRatingCol('rating')\n",
    "\n",
    "# Now let's compute an evaluation metric for our test dataset\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Create an RMSE evaluator using the label and predicted columns\n",
    "reg_eval = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"rating\", metricName=\"rmse\")\n",
    "\n",
    "tolerance = 0.03\n",
    "ranks = [4, 8, 12]\n",
    "errors = [0, 0, 0]\n",
    "models = [0, 0, 0]\n",
    "err = 0\n",
    "min_error = float('inf')\n",
    "best_rank = -1\n",
    "for rank in ranks:\n",
    "  # Set the rank here:\n",
    "  als.setRank(rank)\n",
    "  # Create the model with these parameters.\n",
    "  model = als.fit(training_df)\n",
    "  # Run the model to create a prediction. Predict against the validation_df.\n",
    "  predict_df = model.transform(validation_df)\n",
    "\n",
    "  # Remove NaN values from prediction (due to SPARK-14489)\n",
    "  predicted_ratings_df = predict_df.filter(predict_df.prediction != float('nan'))\n",
    "\n",
    "  # Run the previously created RMSE evaluator, reg_eval, on the predicted_ratings_df DataFrame\n",
    "  error = reg_eval.evaluate(predicted_ratings_df)\n",
    "  errors[err] = error\n",
    "  models[err] = model\n",
    "  print('For rank %s the RMSE is %s' %(rank, error))\n",
    "  if error < min_error:\n",
    "    min_error = error\n",
    "    best_rank = err\n",
    "  err += 1\n",
    "\n",
    "als.setRank(ranks[best_rank])\n",
    "print('The best model was trained with rank %s' % ranks[best_rank])\n",
    "my_model = models[best_rank]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2c) Testing Your Model\n",
    "\n",
    "So far, we used the `training_df` and `validation_df` datasets to select the best model.  Since we used these two datasets to determine what model is best, we cannot use them to test how good the model is; otherwise, we would be very vulnerable to [overfitting](https://en.wikipedia.org/wiki/Overfitting).  To decide how good our model is, we need to use the `test_df` dataset.  We will use the `best_rank` you determined in part (2b) to create a model for predicting the ratings for the test dataset and then we will compute the RMSE.\n",
    "\n",
    "The steps you should perform are:\n",
    "* Run a prediction, using `my_model` as created above, on the test dataset (`test_df`), producing a new `predict_df` DataFrame.\n",
    "* Filter out unwanted NaN values (necessary because of [a bug in Spark](https://issues.apache.org/jira/browse/SPARK-14489)). We've supplied this piece of code for you.\n",
    "* Use the previously created RMSE evaluator, `reg_eval` to evaluate the filtered DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model had a RMSE on the test set of 0.8226051393582325\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL_IN> with the appropriate code\n",
    "# In ML Pipelines, this next step has a bug that produces unwanted NaN values. We\n",
    "# have to filter them out. See https://issues.apache.org/jira/browse/SPARK-14489\n",
    "predict_df = my_model.transform(test_df)\n",
    "\n",
    "# Remove NaN values from prediction (due to SPARK-14489)\n",
    "predicted_test_df = predict_df.filter(predict_df.prediction != float('nan'))\n",
    "\n",
    "# Run the previously created RMSE evaluator, reg_eval, on the predicted_test_df DataFrame\n",
    "test_RMSE = reg_eval.evaluate(predicted_test_df)\n",
    "\n",
    "print('The model had a RMSE on the test set of {0}'.format(test_RMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2d) Comparing Your Model\n",
    "\n",
    "Looking at the RMSE for the results predicted by the model versus the values in the test set is one way to evalute the quality of our model. Another way to evaluate the model is to evaluate the error from a test set where every rating is the average rating for the training set.\n",
    "\n",
    "The steps you should perform are:\n",
    "* Use the `training_df` to compute the average rating across all movies in that training dataset.\n",
    "* Use the average rating that you just determined and the `test_df` to create a DataFrame (`test_for_avg_df`) with a `prediction` column containing the average rating. **HINT**: You'll want to use the `lit()` function,\n",
    "  from `pyspark.sql.functions`, available here as `F.lit()`.\n",
    "* Use our previously created `reg_eval` object to evaluate the `test_for_avg_df` and calculate the RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+\n",
      "|userId|movieId|rating|\n",
      "+------+-------+------+\n",
      "|     1|    307|   3.5|\n",
      "|     1|    481|   3.5|\n",
      "|     1|   1257|   4.5|\n",
      "|     1|   1449|   4.5|\n",
      "|     1|   2134|   4.5|\n",
      "|     1|   2840|   3.0|\n",
      "|     1|   3698|   3.5|\n",
      "|     1|   3826|   2.0|\n",
      "|     1|   3893|   3.5|\n",
      "|     2|    170|   3.5|\n",
      "|     2|    849|   3.5|\n",
      "|     2|   1186|   3.5|\n",
      "|     2|   1296|   4.5|\n",
      "|     2|   2108|   3.5|\n",
      "|     2|   2243|   4.5|\n",
      "|     2|   2352|   4.0|\n",
      "|     2|   2746|   4.0|\n",
      "|     2|   2915|   3.5|\n",
      "|     2|   3363|   4.0|\n",
      "|     3|    828|   4.0|\n",
      "+------+-------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------------------+\n",
      "|       avg(rating)|\n",
      "+------------------+\n",
      "|3.5305381430161074|\n",
      "+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.5305381430161074"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df.show()\n",
    "avg_rating_df = training_df.groupby().avg('rating')\n",
    "avg_rating_df.show()\n",
    "avg_rating_df.collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average rating for movies in the training set is 3.5305381430161074\n",
      "The RMSE on the average set is 1.0662563794506446\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL_IN> with the appropriate code.\n",
    "# Compute the average rating\n",
    "avg_rating_df = training_df.groupby().avg('rating')\n",
    "\n",
    "# Extract the average rating value. (This is row 0, column 0.)\n",
    "training_avg_rating = avg_rating_df.collect()[0][0]\n",
    "\n",
    "print('The average rating for movies in the training set is {0}'.format(training_avg_rating))\n",
    "\n",
    "# Add a column with the average rating\n",
    "test_for_avg_df = test_df.withColumn('prediction', F.lit(training_avg_rating))\n",
    "\n",
    "# Run the previously created RMSE evaluator, reg_eval, on the test_for_avg_df DataFrame\n",
    "test_avg_RMSE = reg_eval.evaluate(test_for_avg_df)\n",
    "\n",
    "print(\"The RMSE on the average set is {0}\".format(test_avg_RMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have code to predict how users will rate movies!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Predictions for Yourself\n",
    "The ultimate goal of this lab exercise is to predict what movies to recommend to yourself.  In order to do that, you will first need to add ratings for yourself to the `ratings_df` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(3a) Your Movie Ratings**\n",
    "\n",
    "To help you provide ratings for yourself, we have included the following code to list the names and movie IDs of the 50 highest-rated movies from `movies_with_500_ratings_or_more` which we created in part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most rated movies:\n",
      "(average rating, movie name, number of reviews, movie ID)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(average=4.4865181711606095, title='Planet Earth II (2016)', count=853, movieId=171011),\n",
       " Row(average=4.458092485549133, title='Planet Earth (2006)', count=1384, movieId=159817),\n",
       " Row(average=4.424188001918387, title='Shawshank Redemption, The (1994)', count=97999, movieId=318),\n",
       " Row(average=4.399898373983739, title='Band of Brothers (2001)', count=984, movieId=170705),\n",
       " Row(average=4.350558659217877, title='Black Mirror: White Christmas (2014)', count=1074, movieId=174053),\n",
       " Row(average=4.332892749244713, title='Godfather, The (1972)', count=60904, movieId=858),\n",
       " Row(average=4.291958829205532, title='Usual Suspects, The (1995)', count=62180, movieId=50),\n",
       " Row(average=4.2630353697749195, title='Godfather: Part II, The (1974)', count=38875, movieId=1221),\n",
       " Row(average=4.257501817775044, title=\"Schindler's List (1993)\", count=71516, movieId=527),\n",
       " Row(average=4.2541157909178215, title='Seven Samurai (Shichinin no samurai) (1954)', count=14578, movieId=2019),\n",
       " Row(average=4.237075455914338, title='12 Angry Men (1957)', count=17931, movieId=1203),\n",
       " Row(average=4.230798598634567, title='Rear Window (1954)', count=22264, movieId=904),\n",
       " Row(average=4.230663235786717, title='Fight Club (1999)', count=65678, movieId=2959),\n",
       " Row(average=4.222920272160452, title=\"One Flew Over the Cuckoo's Nest (1975)\", count=42181, movieId=1193),\n",
       " Row(average=4.210098086509085, title='Casablanca (1942)', count=31095, movieId=912),\n",
       " Row(average=4.208876000542667, title='Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1964)', count=29484, movieId=750),\n",
       " Row(average=4.2076678004047015, title='Spirited Away (Sen to Chihiro no kamikakushi) (2001)', count=23227, movieId=5618),\n",
       " Row(average=4.20375939849624, title='Third Man, The (1949)', count=7980, movieId=1212),\n",
       " Row(average=4.202861952861953, title='Whiplash (2013)', count=594, movieId=166024),\n",
       " Row(average=4.201752440106477, title='Paths of Glory (1957)', count=4508, movieId=1178),\n",
       " Row(average=4.201091113037271, title='North by Northwest (1959)', count=19613, movieId=908),\n",
       " Row(average=4.199844881075491, title='Lives of Others, The (Das leben der Anderen) (2006)', count=9670, movieId=44555),\n",
       " Row(average=4.199261675824176, title='Double Indemnity (1944)', count=5824, movieId=3435),\n",
       " Row(average=4.195425943852856, title='Sunset Blvd. (a.k.a. Sunset Boulevard) (1950)', count=8264, movieId=922),\n",
       " Row(average=4.184896558122275, title='City of God (Cidade de Deus) (2002)', count=21558, movieId=6016),\n",
       " Row(average=4.179297597042514, title='Yojimbo (1961)', count=4328, movieId=3030),\n",
       " Row(average=4.177603383981745, title='Goodfellas (1990)', count=35934, movieId=1213),\n",
       " Row(average=4.173971387139363, title='Pulp Fiction (1994)', count=92406, movieId=296),\n",
       " Row(average=4.173755615654545, title='Dark Knight, The (2008)', count=44741, movieId=58559),\n",
       " Row(average=4.170276988636363, title='All About Eve (1950)', count=5632, movieId=926),\n",
       " Row(average=4.163581345674618, title='Notorious (1946)', count=5618, movieId=930),\n",
       " Row(average=4.163178366364141, title='Life Is Beautiful (La Vita è bella) (1997)', count=26995, movieId=2324),\n",
       " Row(average=4.1629897528631705, title='Inception (2010)', count=41475, movieId=79132),\n",
       " Row(average=4.161491297468355, title='M (1931)', count=5056, movieId=1260),\n",
       " Row(average=4.159891808346213, title='Big Sleep, The (1946)', count=6470, movieId=1284),\n",
       " Row(average=4.153844395162213, title='Memento (2000)', count=43739, movieId=4226),\n",
       " Row(average=4.153380031131865, title='To Kill a Mockingbird (1962)', count=17988, movieId=1207),\n",
       " Row(average=4.1524313561098305, title='Chinatown (1974)', count=19084, movieId=1252),\n",
       " Row(average=4.15141241652351, title='Silence of the Lambs, The (1991)', count=87899, movieId=593),\n",
       " Row(average=4.15122820176261, title='Touch of Evil (1958)', count=5333, movieId=1248),\n",
       " Row(average=4.150782937365011, title='Thin Man, The (1934)', count=3704, movieId=950),\n",
       " Row(average=4.149695428470046, title='Matrix, The (1999)', count=84545, movieId=2571),\n",
       " Row(average=4.148407904167093, title='My Neighbor Totoro (Tonari no Totoro) (1988)', count=9767, movieId=5971),\n",
       " Row(average=4.14835070718935, title='Monty Python and the Holy Grail (1975)', count=40866, movieId=1136),\n",
       " Row(average=4.148115687992989, title='Ran (1985)', count=5705, movieId=1217),\n",
       " Row(average=4.141628504411929, title='Sting, The (1973)', count=17906, movieId=1234),\n",
       " Row(average=4.14138438880707, title='Harakiri (Seppuku) (1962)', count=679, movieId=26082),\n",
       " Row(average=4.1408824391674, title='American History X (1998)', count=34110, movieId=2329),\n",
       " Row(average=4.1392657621707905, title='Piper (2016)', count=1253, movieId=160718),\n",
       " Row(average=4.13421052631579, title='It Happened One Night (1934)', count=4750, movieId=905)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Most rated movies:')\n",
    "print('(average rating, movie name, number of reviews, movie ID)')\n",
    "display(movies_with_500_ratings_or_more.orderBy(movies_with_500_ratings_or_more['average'].desc()).take(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user ID 0 is unassigned, so we will use it for your ratings. We set the variable `my_user_ID` to 0 for you. Next, create a new DataFrame called `my_ratings_df`, with your ratings for at least 10 movie ratings. Each entry should be formatted as `(my_user_id, movieID, rating)`.  As in the original dataset, ratings should be between 1 and 5 (inclusive). If you have not seen at least 10 of these movies, you can increase the parameter passed to `take()` in the above cell until there are 10 movies that you have seen (or you can also guess what your rating would be for movies you have not seen)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My movie ratings:\n",
      "+------+-------+------+\n",
      "|userId|movieId|rating|\n",
      "+------+-------+------+\n",
      "|     0|    318|   4.7|\n",
      "|     0|    541|   4.2|\n",
      "|     0|    111|   4.3|\n",
      "|     0|   4993|   4.5|\n",
      "|     0|    608|   4.6|\n",
      "|     0|   7153|   4.8|\n",
      "|     0|   1207|   4.8|\n",
      "|     0|   4226|   4.5|\n",
      "|     0|  68157|   4.6|\n",
      "|     0|   1210|   4.7|\n",
      "+------+-------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "from pyspark.sql import Row\n",
    "my_user_id = 0\n",
    "\n",
    "# Note that the movie IDs are the *last* number on each line. A common error was to use the number of ratings as the movie ID.\n",
    "my_rated_movies = [\n",
    "#      <FILL IN>\n",
    "    (my_user_id, 318, 4.7),\n",
    "    (my_user_id, 541, 4.2),\n",
    "    (my_user_id, 111, 4.3),\n",
    "    (my_user_id, 4993, 4.5),\n",
    "    (my_user_id, 608, 4.6),\n",
    "    (my_user_id, 7153, 4.8),\n",
    "    (my_user_id, 1207, 4.8),\n",
    "    (my_user_id, 4226, 4.5),\n",
    "    (my_user_id, 68157, 4.6),\n",
    "    (my_user_id, 1210, 4.7),\n",
    "    \n",
    "     # The format of each line is (my_user_id, movie ID, your rating)\n",
    "     # For example, to give the movie \"Star Wars: Episode IV - A New Hope (1977)\" a five rating, you would add the following line:\n",
    "     #   (my_user_id, 260, 5),\n",
    "]\n",
    "\n",
    "my_ratings_df = sqlContext.createDataFrame(my_rated_movies, ['userId','movieId','rating'])\n",
    "print('My movie ratings:')\n",
    "display(my_ratings_df.limit(10).show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3b) Add Your Movies to Training Dataset\n",
    "\n",
    "Now that you have ratings for yourself, you need to add your ratings to the `training` dataset so that the model you train will incorporate your preferences.  Spark's [unionAll()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.unionAll) transformation combines two DataFrames; use `unionAll()` to create a new training dataset that includes your ratings and the data in the original training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset now has 10 more entries than the original training dataset\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# training_with_my_ratings_df = <FILL IN>\n",
    "training_with_my_ratings_df = training_df.unionAll(my_ratings_df)\n",
    "\n",
    "print (('The training dataset now has %s more entries than the original training dataset' %\n",
    "       (training_with_my_ratings_df.count() - training_df.count())))\n",
    "assert (training_with_my_ratings_df.count() - training_df.count()) == my_ratings_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3c) Train a Model with Your Ratings\n",
    "\n",
    "Now, train a model with your ratings added and the parameters you used in in part (2b) and (2c). Mke sure you include **all** of the parameters.\n",
    "\n",
    "**Note**: This cell will take about 30 seconds to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "# Reset the parameters for the ALS object.\n",
    "als.setPredictionCol(\"prediction\")\\\n",
    "   .setMaxIter(5)\\\n",
    "   .setSeed(seed)\\\n",
    "   .setRegParam(0.1)\\\n",
    "   .setUserCol('userId')\\\n",
    "   .setItemCol('movieId')\\\n",
    "   .setRatingCol('rating')\\\n",
    "   .setRank(12)\n",
    "\n",
    "# Create the model with these parameters.\n",
    "my_ratings_model = als.fit(training_with_my_ratings_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3d) Check RMSE for the New Model with Your Ratings\n",
    "\n",
    "Compute the RMSE for this new model on the test set.\n",
    "* Run your model (the one you just trained) against the test data set in `test_df`.\n",
    "* Then, use our previously-computed `reg_eval` object to compute the RMSE of your ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model had a RMSE on the test set of 0.8230874724252297\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "my_predict_df = my_ratings_model.transform(test_df)\n",
    "\n",
    "# Remove NaN values from prediction (due to SPARK-14489)\n",
    "predicted_test_my_ratings_df = my_predict_df.filter(my_predict_df.prediction != float('nan'))\n",
    "\n",
    "# Run the previously created RMSE evaluator, reg_eval, on the predicted_test_my_ratings_df DataFrame\n",
    "test_RMSE_my_ratings = reg_eval.evaluate(predicted_test_my_ratings_df)\n",
    "print('The model had a RMSE on the test set of {0}'.format(test_RMSE_my_ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+\n",
      "|userId|movieId|rating|prediction|\n",
      "+------+-------+------+----------+\n",
      "|   540|   2366|   4.0| 3.3640974|\n",
      "|  1824|   2122|   3.0| 2.6314788|\n",
      "|  2038|   1088|   1.0| 2.7639434|\n",
      "|  2564|   1591|   3.0| 2.9044042|\n",
      "|  4259|   2366|   1.0|  4.305943|\n",
      "|  5335|   1591|   2.5| 2.5263286|\n",
      "|  5427|   4519|   4.0| 2.8550644|\n",
      "|  5897|   8638|   4.5| 3.4569566|\n",
      "|  6342|   1088|   3.5| 3.5085568|\n",
      "|  6435|   2366|   2.5|  2.374332|\n",
      "+------+-------+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_predict_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3e) Predict Your Ratings\n",
    "\n",
    "So far, we have only computed the error of the model.  Next, let's predict what ratings you would give to the movies that you did not already provide ratings for.\n",
    "\n",
    "The steps you should perform are:\n",
    "* Filter out the movies you already rated manually. (Use the `my_rated_movie_ids` variable.) Put the results in a new `not_rated_df`.\n",
    "\n",
    "   **Hint**: The [Column.isin()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column.isin)\n",
    "   method, as well as the `~` (\"not\") DataFrame logical operator, may come in handy here. Here's an example of using `isin()`:\n",
    "\n",
    "```\n",
    "    > df1 = sqlContext.createDataFrame([(\"Jim\", 10), (\"Julie\", 9), (\"Abdul\", 20), (\"Mireille\", 19)], [\"name\", \"age\"])\n",
    "    > df1.show()\n",
    "    +--------+---+\n",
    "    |    name|age|\n",
    "    +--------+---+\n",
    "    |     Jim| 10|\n",
    "    |   Julie|  9|\n",
    "    |   Abdul| 20|\n",
    "    |Mireille| 19|\n",
    "    +--------+---+\n",
    "\n",
    "    > names_to_delete = [\"Julie\", \"Abdul\"] # this is just a Python list\n",
    "    > df2 = df1.filter(~ df1[\"name\"].isin(names_to_delete)) # \"NOT IN\"\n",
    "    > df2.show()\n",
    "    +--------+---+\n",
    "    |    name|age|\n",
    "    +--------+---+\n",
    "    |     Jim| 10|\n",
    "    |Mireille| 19|\n",
    "    +--------+---+\n",
    "```\n",
    "\n",
    "* Transform `not_rated_df` into `my_unrated_movies_df` by:\n",
    "    - renaming the \"ID\" column to \"movieId\"\n",
    "    - adding a \"userId\" column with the value contained in the `my_user_id` variable defined above.\n",
    "\n",
    "* Create a `predicted_ratings_df` DataFrame by applying `my_ratings_model` to `my_unrated_movies_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID=1, title='Toy Story (1995)'), Row(ID=2, title='Jumanji (1995)')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to fix this issue \n",
    "'''Detected implicit cartesian product for LEFT OUTER join between logical plans'''\n",
    "spark.conf.set(\"spark.sql.crossJoin.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL_IN> with the appropriate code\n",
    "\n",
    "# Create a list of my rated movie IDs\n",
    "my_rated_movie_ids = [x[1] for x in my_rated_movies]\n",
    "\n",
    "# Filter out the movies I already rated.\n",
    "not_rated_df = movies_df.filter(~ movies_df.ID.isin(my_rated_movie_ids))\n",
    "\n",
    "# Rename the \"ID\" column to be \"movieId\", and add a column with my_user_id as \"userId\".\n",
    "my_unrated_movies_df = not_rated_df.withColumn(\"userId\",F.lit(my_user_id)).withColumnRenamed(\"ID\", \"movieId\")\n",
    "\n",
    "\n",
    "# Use my_rating_model to predict ratings for the movies that I did not manually rate.\n",
    "raw_predicted_ratings_df = my_ratings_model.transform(my_unrated_movies_df)\n",
    "\n",
    "predicted_ratings_df = raw_predicted_ratings_df.filter(raw_predicted_ratings_df['prediction'] != float('nan'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+----------+\n",
      "|movieId|               title|userId|prediction|\n",
      "+-------+--------------------+------+----------+\n",
      "|      1|    Toy Story (1995)|     0|  4.277744|\n",
      "|      2|      Jumanji (1995)|     0| 3.2448394|\n",
      "|      3|Grumpier Old Men ...|     0| 2.9605477|\n",
      "|      4|Waiting to Exhale...|     0|  2.414985|\n",
      "|      5|Father of the Bri...|     0| 2.7641041|\n",
      "|      6|         Heat (1995)|     0| 4.1612363|\n",
      "|      7|      Sabrina (1995)|     0| 3.2049425|\n",
      "|      8| Tom and Huck (1995)|     0| 2.7173464|\n",
      "|      9| Sudden Death (1995)|     0| 2.5871289|\n",
      "|     10|    GoldenEye (1995)|     0| 3.5712993|\n",
      "|     11|American Presiden...|     0| 3.6223898|\n",
      "|     12|Dracula: Dead and...|     0|  2.494406|\n",
      "|     13|        Balto (1995)|     0|  3.308155|\n",
      "|     14|        Nixon (1995)|     0| 3.5381107|\n",
      "|     15|Cutthroat Island ...|     0| 2.5112336|\n",
      "|     16|       Casino (1995)|     0| 4.1299467|\n",
      "|     17|Sense and Sensibi...|     0| 3.9675198|\n",
      "|     18|   Four Rooms (1995)|     0|  3.486565|\n",
      "|     19|Ace Ventura: When...|     0| 2.5089984|\n",
      "|     20|  Money Train (1995)|     0| 2.6330144|\n",
      "+-------+--------------------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_predicted_ratings_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3f) Predict Your Ratings\n",
    "\n",
    "We have our predicted ratings. Now we can print out the 25 movies with the highest predicted ratings.\n",
    "\n",
    "The steps you should perform are:\n",
    "* Join your `predicted_ratings_df` DataFrame with the `movie_names_with_avg_ratings_df` DataFrame to obtain the ratings counts for each movie.\n",
    "* Sort the resulting DataFrame (`predicted_with_counts_df`) by predicted rating (highest ratings first), and remove any ratings with a count of 75 or less.\n",
    "* Print the top 25 movies that remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My 25 highest rated movies as predicted (for movies with more than 75 reviews):\n",
      "+------------------+---------------------------------------------------------------------------+-------+-----+\n",
      "|average           |title                                                                      |movieId|count|\n",
      "+------------------+---------------------------------------------------------------------------+-------+-----+\n",
      "|4.4865181711606095|Planet Earth II (2016)                                                     |171011 |853  |\n",
      "|4.458092485549133 |Planet Earth (2006)                                                        |159817 |1384 |\n",
      "|4.399898373983739 |Band of Brothers (2001)                                                    |170705 |984  |\n",
      "|4.350558659217877 |Black Mirror: White Christmas (2014)                                       |174053 |1074 |\n",
      "|4.343949044585988 |Cosmos                                                                     |171495 |157  |\n",
      "|4.339667458432304 |The Godfather Trilogy: 1972-1990 (1992)                                    |172591 |421  |\n",
      "|4.332892749244713 |Godfather, The (1972)                                                      |858    |60904|\n",
      "|4.291958829205532 |Usual Suspects, The (1995)                                                 |50     |62180|\n",
      "|4.263888888888889 |Black Mirror                                                               |176601 |180  |\n",
      "|4.2630353697749195|Godfather: Part II, The (1974)                                             |1221   |38875|\n",
      "|4.261904761904762 |Last Year's Snow Was Falling (1983)                                        |172577 |126  |\n",
      "|4.257501817775044 |Schindler's List (1993)                                                    |527    |71516|\n",
      "|4.2541157909178215|Seven Samurai (Shichinin no samurai) (1954)                                |2019   |14578|\n",
      "|4.244031830238727 |Over the Garden Wall (2013)                                                |163809 |377  |\n",
      "|4.23943661971831  |Sherlock - A Study in Pink (2010)                                          |185135 |213  |\n",
      "|4.237075455914338 |12 Angry Men (1957)                                                        |1203   |17931|\n",
      "|4.236389684813753 |Blue Planet II (2017)                                                      |179135 |349  |\n",
      "|4.230798598634567 |Rear Window (1954)                                                         |904    |22264|\n",
      "|4.230663235786717 |Fight Club (1999)                                                          |2959   |65678|\n",
      "|4.222920272160452 |One Flew Over the Cuckoo's Nest (1975)                                     |1193   |42181|\n",
      "|4.2137767220902616|The Blue Planet (2001)                                                     |142115 |421  |\n",
      "|4.210098086509085 |Casablanca (1942)                                                          |912    |31095|\n",
      "|4.208876000542667 |Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1964)|750    |29484|\n",
      "|4.2076678004047015|Spirited Away (Sen to Chihiro no kamikakushi) (2001)                       |5618   |23227|\n",
      "|4.20375939849624  |Third Man, The (1949)                                                      |1212   |7980 |\n",
      "+------------------+---------------------------------------------------------------------------+-------+-----+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL_IN> with the appropriate code\n",
    "\n",
    "predicted_with_counts_df = (predicted_ratings_df\n",
    "                                .join(movie_names_with_avg_ratings_df, predicted_ratings_df.movieId == movie_names_with_avg_ratings_df.movieId)\n",
    "                                .select(\"average\", movie_names_with_avg_ratings_df[\"title\"], movie_names_with_avg_ratings_df[\"movieId\"], \"count\"))\n",
    "\n",
    "predicted_highest_rated_movies_df = predicted_with_counts_df.orderBy(predicted_with_counts_df['average'].desc()).cache()\n",
    "\n",
    "print ('My 25 highest rated movies as predicted (for movies with more than 75 reviews):')\n",
    "predicted_highest_rated_movies_df.filter(\"count >= 75\").show(25, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+-------+-----+\n",
      "|           average|               title|movieId|count|\n",
      "+------------------+--------------------+-------+-----+\n",
      "|2.6466656422864165|        Spawn (1997)|   1591| 6508|\n",
      "|3.2480141843971633|Dirty Dancing (1987)|   1088|14100|\n",
      "|2.6475240715268225|Children of the C...|   2122| 2908|\n",
      "| 2.978014656895403|Hellbound: Hellra...|   3918| 1501|\n",
      "|3.3402777777777777|Land Before Time,...|   4519| 2664|\n",
      "|3.9713673548889754|Before Sunset (2004)|   8638| 5134|\n",
      "| 2.711587708066581|High School High ...|    833| 1562|\n",
      "|2.9728327982217833|     Candyman (1992)|   1342| 4049|\n",
      "| 2.907754010695187|Awfully Big Adven...|    148|  374|\n",
      "| 3.473642753271934|    King Kong (1933)|   2366| 8252|\n",
      "|3.1914893617021276|Dogs in Space (1987)|   4101|   47|\n",
      "|3.6663385826771653| High Society (1956)|   6357|  508|\n",
      "|              3.25|   Bad Ronald (1974)|  83250|   18|\n",
      "| 3.295990566037736|What Happened Was...|    496|  424|\n",
      "| 3.598360655737705|Welcome to Leith ...| 142084|   61|\n",
      "|2.9696969696969697|  Moonwalkers (2015)| 150604|   66|\n",
      "|3.3378378378378377|Friday Night (Ven...|   7880|   37|\n",
      "|  3.77027027027027|When the Last Swo...|  27760|   37|\n",
      "|2.7058823529411766|Love and the City...| 172523|   17|\n",
      "|               2.5|   The Dancer (2000)| 173691|    3|\n",
      "+------------------+--------------------+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_with_counts_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "name": "W6L1: Recommender Systems - ALS Prediction",
  "notebookId": 2298298480838425
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
